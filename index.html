<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LLMLight</title>
  <link rel="icon" type="image/x-icon" href="static/images/llmlight.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LLMLight: Large Language Models as Traffic Signal Control Agents</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=K4YI7ksAAAAJ&hl=en" target="_blank">Siqi Lai</a>,</span>
                <span class="author-block">
                  <a href="https://xzbill.top/zhaoxu/" target="_blank">Zhao Xu</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=lSi3CIoAAAAJ&hl=en" target="_blank">Weijia Zhang</a>,</span>
                  <span class="author-block">
                    <a href="https://raymondhliu.github.io/" target="_blank">Hao Liu</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=cVDF1tkAAAAJ&hl=en" target="_blank">Hui Xiong</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The Hong Kong University of Science and Technology (Guangzhou)<br>Under Review</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2312.16044.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
<!--                    <span class="link-block">-->
<!--                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
<!--                      class="external-link button is-normal is-rounded is-dark">-->
<!--                      <span class="icon">-->
<!--                        <i class="fas fa-file-pdf"></i>-->
<!--                      </span>-->
<!--                      <span>Supplementary</span>-->
<!--                    </a>-->
<!--                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/usail-hkust/LLMTSCS" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2312.16044" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/Demo.mov"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        The demonstration video of LLMLight (powered by LightGPT).
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional methods in TSC, primarily based on transportation engineering and reinforcement learning (RL), often exhibit limitations in generalization across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments on nine real-world and synthetic datasets showcase the remarkable effectiveness, generalization ability, and interpretability of LLMLight against nine transportation-based and RL-based baselines.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Motivation</h2>
          <div class="level-set has-text-justified">
            <p>
              Existing research on TSC has primarily fallen into two categories: transportation and Reinforcement Learning~(RL)-based approaches. Transportation methods primarily focus on crafting efficient heuristic algorithms, dynamically adapting traffic signal configurations based on lane-level traffic conditions. However, these methods heavily rely on manual design, demanding substantial human effort. The emergence of deep neural networks (DNNs) led to the introduction of deep RL-based techniques to address this challenge. These approaches have exhibited remarkable performance across various traffic scenarios. Nevertheless, RL-based methods also present several drawbacks. Primarily, they may struggle with limited generalization ability, particularly when transferring to larger-scale road networks or under highly uncommon scenarios (e.g., extreme high-traffic situations), as their training data only covers limited traffic situations. Additionally, RL-based methods lack interpretability due to the black-box nature of DNNs, which makes it hard to explain the rationale behind their control actions under specific traffic conditions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Challenge 1</h2>
          <div class="level-set has-text-justified">
            <p>
              First, LLMs are typically pre-trained on large-scale natural language corpora and rarely incorporate non-textual traffic data, such as sensor readings and GPS trajectories. Despite their generalization capability across various tasks and domains, an inherent gap exists between real-time traffic data and linguistic understanding. The first challenge lies in enabling LLMs to comprehend real-time traffic dynamics and effectively interact with the traffic environment, which is critical for effective LLM-based traffic signal control.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Challenge 2</h2>
          <div class="level-set has-text-justified">
            <p>
              Second, selecting and developing an effective LLM for TSC poses another significant challenge. Generalist LLMs often lack specific domain knowledge and are prone to hallucination problems in professional fields. Although state-of-the-art LLMs such as GPT-4 demonstrate promising generalization abilities, their closed-source nature and substantial usage costs pose barriers to their optimization for real-time TSC tasks. Consequently, building a specialized LLM tailored for TSC tasks is crucial to deliver more effective and human-aligned control policies.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Workflow</h2>
          <div class="level-set has-text-justified">
            <p>
              To this end, we introduce LLMLight, a traffic signal control agent framework based on LLMs, as depicted in the figure. Specifically, we consider TSC as a partially observable Markov Game, where each agent, armed with an LLM, manages the traffic light at an intersection. At each signal-switching time step, the agent collects traffic conditions of the target intersection and transforms them into human-readable text as real-time observation. Additionally, we incorporate task descriptions enriched with commonsense knowledge about a control strategy to aid the LLM's understanding of traffic management tasks. The combined real-time observation, task description, and control action space form a knowledgeable prompt that guides the LLM's decision-making. Then, the agent leverages Chain-of-Thought (CoT) reasoning to determine the optimal traffic signal configuration for the subsequent time step.
            </p>
          </div>
          <img src="static/images/workflow.png" alt="workflow" class="center-image"/>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">LightGPT Training</h2>
          <div class="level-set has-text-justified">
            <p>
              Furthermore, we construct a specialized LLM, LightGPT, to enhance the LLMLight framework. On the one hand, we propose imitation fine-tuning to let the specialized LLM exploit high-quality control actions and underlying rationales derived from GPT-4. On the other hand, we introduce a policy refinement process that utilizes a well-trained critic model to evaluate and improve the agent's actions. The optimized LightGPT produces more effective control policies and showcases remarkable generalization ability across diverse traffic scenarios in a more cost-effective manner than GPT-4.
            </p>
          </div>
          <img src="static/images/training.png" alt="training" class="center-image"/>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Comparison Between LLMLight and Traditional Methods</h2>
          <div class="level-set has-text-justified">
          </div>
          <img src="static/images/comparison.png" alt="training" class="center-image"/>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="section is-small">
    <div class="container is-max-desktop">
    <div class="container">
      <h2 class="title is-3">Generalization Ability</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/transfer_llmlight_jinan.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Figure 1: The transferability on the Jinan dataset.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/transfer_llmlight_hangzhou.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Figure 2: The transferability on the Hangzhou dataset.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/scalability_att.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Figure 3: The scalability (in Average Travel Time) on the New York dataset.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/scalability_awt.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Figure 4: The scalability (in Average Waiting Time) on the New York dataset.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/extreme.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Figure 5: The performance under extreme high-traffic scenarios.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
    <div class="container">
      <h2 class="title is-3">Prompt & Decision-making Process</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/prompt.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Figure 1: The prompt template of LLMLight.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/reasoning.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Figure 2: The action execution process of LLMLight.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{lai2024llmlight,
      title={LLMLight: Large Language Models as Traffic Signal Control Agents},
      author={Siqi Lai and Zhao Xu and Weijia Zhang and Hao Liu and Hui Xiong},
      year={2024},
      eprint={2312.16044},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p class="has-text-centered">
            This work is developed by USAIL Group of the Hong Kong University of Science and Technology.
          </p>
          <div class="image-container">
            <img src="static/images/logos.png" alt="Usail logo" class="center-image"/>
          </div>
        </div>
      </div>
    </div>
  </div>
</footer>

<style>
  .image-container {
    display: flex;
    justify-content: center;
    height: 100px;
  }
</style>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
